{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.signal as spsig\n",
    "\n",
    "# standard python, say geologists\n",
    "import obspy\n",
    "\n",
    "# standard python, say reasonable people\n",
    "import resampy # for resampling\n",
    "import librosa # for writing wav files\n",
    "\n",
    "# custom functions that we need\n",
    "import sys\n",
    "sys.path.append('./classes/')\n",
    "import CMMR_class\n",
    "\n",
    "import datetime # just to check how long the algorithm takes\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook # this is cool... but very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 4 speakers\n"
     ]
    }
   ],
   "source": [
    "n_speakers = 4 # number of speakers\n",
    "print('Working with ' + str(n_speakers) + ' speakers')\n",
    "sound_output_dir = '../sounds/output/'\n",
    "\n",
    "# data sampling (here data = event times)\n",
    "data_dt = 1 # sampling period of the data, in seconds\n",
    "data_sr = 1/data_dt\n",
    "\n",
    "# audio sampling\n",
    "audio_final_sr = 44100 # when resampling and exporting audio\n",
    "audio_duration = 60 # duration of final audio tracks, in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_dir = '../catalogs/'\n",
    "catalog_name = 'EQ_Eruption_Catalog_4_1_2018_8_30_2018_HVO.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(catalog_dir+catalog_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're considering 41328 events\n",
      "Data time goes from 1522565943\n",
      "\t\t to 1535668277\n",
      "        by steps of 1 secs.\n",
      "... that is 13102334 seconds\n",
      "... that is 3639.54 hours\n",
      "... that is 151.65 day\n",
      "Our data array has 13102334 samples\n"
     ]
    }
   ],
   "source": [
    "n_events = len(catalog['latitude'])\n",
    "##### CHANGE THIS\n",
    "#n_events = 1000\n",
    "#####\n",
    "print('We\\'re considering ' + str(n_events) + ' events')\n",
    "\n",
    "ev_lat = np.array(catalog['latitude'])[0:n_events]\n",
    "ev_long = np.array(catalog['longitude'])[0:n_events]\n",
    "ev_mag = np.array(catalog['mag'])[0:n_events]\n",
    "minmag = np.amin(ev_mag); maxmag = np.amax(ev_mag)\n",
    "\n",
    "ref_pt = np.array([min(ev_long),min(ev_lat)])  # pour déterminer le rapport de distance entre 1 degré de lon et un degré de lat\n",
    "ev_X = (ev_long - ref_pt[0])*2.*np.pi/360.*6371.*np.sin(2.*np.pi/360.*(90.-ev_lat))\n",
    "ev_Y = (ev_lat - ref_pt[1])*2.*np.pi/360.*6371.\n",
    "\n",
    "ev_times = np.zeros(n_events)\n",
    "for i in range(n_events):\n",
    "    ev_times[i] = obspy.UTCDateTime(catalog['time'][i]).timestamp\n",
    "    \n",
    "# obspy.UTCDateTime('2018-05-20T21:50:07.310Z').timestamp\n",
    "\n",
    "# set some variables for the track (in the seismic time domain)\n",
    "data_t_start = obspy.UTCDateTime(ev_times[0]).timestamp\n",
    "data_t_end = obspy.UTCDateTime(ev_times[-1]).timestamp\n",
    "data_duration = data_t_end-data_t_start\n",
    "data_t = np.arange(data_t_start,data_t_end,data_dt)\n",
    "\n",
    "print('Data time goes from ' + '{:.0f}'.format(data_t_start) + '\\n\\t\\t to ' + '{:.0f}'.format(data_t_end) + '\\n        by steps of ' + str(data_dt) + ' secs.')\n",
    "print('... that is ' + '{:.0f}'.format(data_t_end-data_t_start) + ' seconds')\n",
    "print('... that is ' + '{:.2f}'.format((data_t_end-data_t_start)/3600) + ' hours')\n",
    "print('... that is ' + '{:.2f}'.format((data_t_end-data_t_start)/3600/24) + ' day')\n",
    "\n",
    "print('Our data array has ' + str(len(data_t)) + ' samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATLON = 0 # set it to 1 if you want to place spks & stations according to latitude & longitude, otherwise it's in kms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define speed factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll be working with an audio sampling rate of 218372.2 Hz\n"
     ]
    }
   ],
   "source": [
    "speedfactor = data_duration/audio_duration\n",
    "audio_working_sr = speedfactor*data_sr\n",
    "print('We\\'ll be working with an audio sampling rate of ' + '{:.1f}'.format(audio_working_sr) + ' Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## place speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center of speaker array is @[-155.14408255, 19.36258315]\n"
     ]
    }
   ],
   "source": [
    "if LATLON:\n",
    "    array_center = [(max(ev_long)+min(ev_long))/2,(max(ev_lat)+min(ev_lat))/2]\n",
    "else:\n",
    "    array_center = [(max(ev_X)+min(ev_X))/2,(max(ev_Y)+min(ev_Y))/2]\n",
    "print('Center of speaker array is @' + str(array_center))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's draw a polygon of radius 0.19552170647158879\n"
     ]
    }
   ],
   "source": [
    "if LATLON:\n",
    "    radius = np.sqrt((min(ev_long)-array_center[0])**2+(min(ev_lat)-array_center[1])**2) # distance between speakers & center\n",
    "else:\n",
    "    radius = np.sqrt((min(ev_X)-array_center[0])**2+(min(ev_Y)-array_center[1])**2) # distance between speakers & center\n",
    "radius = radius/2\n",
    "print('Let\\'s draw a polygon of radius ' + str(radius))\n",
    "\n",
    "angle_shift = -np.pi/4 # rotate the array (in rads) around the center\n",
    "[x_speakers,y_speakers] = CMMR_class.Coordinates_Polygon(array_center,n_speakers,radius,angle_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot events and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display params\n",
    "arrcent_marksize = 5\n",
    "arrcent_markcol = 'r'\n",
    "arrcent_marktype = 's'\n",
    "arrcent_style = 'none'\n",
    "\n",
    "spk_marksize = 10\n",
    "spk_markcol = 'b'\n",
    "spk_marktype = 'v'\n",
    "spk_style = 'none'\n",
    "spk_num_offset = 1 # offset for plotting the speaker number\n",
    "spk_num_size = 20 # font size for speaker number\n",
    "\n",
    "ev_markcol = 'k'\n",
    "ev_marktype = '.'\n",
    "ev_marksize = 3\n",
    "ev_style = 'none'\n",
    "\n",
    "#plot!\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "#plt.plot(ev_long,ev_lat,color=ev_markcol,marker=ev_marktype,markersize=ev_marksize,linestyle=ev_style)\n",
    "if LATLON:\n",
    "    plt.scatter(ev_long,ev_lat,marker=ev_marktype,s=ev_marksize,cmap='inferno',c=ev_times)\n",
    "else:\n",
    "    plt.scatter(ev_X,ev_Y,marker=ev_marktype,s=ev_marksize,cmap='inferno',c=ev_times)\n",
    "cbar = plt.colorbar(fraction=0.02, pad=0.02)\n",
    "cbar.set_label('date (time stamp)')\n",
    "plt.plot(array_center[0],array_center[1],color=arrcent_markcol,marker=arrcent_marktype,markersize=arrcent_marksize,linestyle=arrcent_style)\n",
    "for k in range(n_speakers):\n",
    "    plt.plot(x_speakers[k],y_speakers[k],color=spk_markcol,marker=spk_marktype,markersize=spk_marksize,linestyle=spk_style)\n",
    "    plt.text(x_speakers[k]+spk_num_offset,y_speakers[k]+spk_num_offset,str(k+1),color=spk_markcol, fontsize=spk_num_size)\n",
    "plt.grid()\n",
    "plt.gca().set_aspect('equal')\n",
    "if LATLON:\n",
    "    plt.ylabel('Latitude (°N)'), plt.xlabel('Longitude (°W)')\n",
    "else:\n",
    "    plt.ylabel('Y (kms)'), plt.xlabel('X (kms)')\n",
    "fig.savefig('../output/Kilauea_MAP.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the data track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a series of ones at the event times (time stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_track = np.zeros(len(data_t))\n",
    "ev_inds = (ev_times-data_t_start)*data_sr # event occurrence times as indices in the data array\n",
    "\n",
    "for i in range(n_events):\n",
    "    index = int(ev_inds[i]) # round to nearest integer, so precision in event times = data sampling period\n",
    "    data_track[index] = 1\n",
    "\n",
    "CHECK = 0\n",
    "if CHECK:\n",
    "    fig, ax = plt.subplots(figsize=(14,7))\n",
    "    ax.plot(data_t,data_track)\n",
    "    plt.grid()\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    \n",
    "    # check if events are at the right place\n",
    "    for this in ev_times:\n",
    "        #print('{:.0f}'.format(this),end='\\t')\n",
    "        plt.plot(this,1,'xr',linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the audio tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate the audio click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_dir = '../sounds/base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKSOUND = 0\n",
    "\n",
    "#CLICK = 1 # just an ideal impulse, i.e. 1 at 1 point only\n",
    "#CLICK = 2 # Arthur's descending chirp with skew gaussian envelope\n",
    "CLICK = 2.5 # Arthur's descending chirp with skew gaussian envelope, frequency decreases with magnitude\n",
    "#CLICK = 3 # Ben's \n",
    "\n",
    "if CLICK == 1:\n",
    "    click_suffix = '_ClickDirac_'\n",
    "    click = 1\n",
    "elif CLICK == 2:\n",
    "    click_suffix = '_ClickDescChirp_'\n",
    "    \n",
    "    ## ALL THAT FOLLOWS IS IN AUDIO TIME\n",
    "    # say a click is a descending chirp\n",
    "    click_dur = 0.15 # in seconds \n",
    "    t_click = np.arange(0,click_dur,1/audio_working_sr)\n",
    "    fstart = 300 # start frequency\n",
    "    fend = .5*fstart\n",
    "    click = spsig.chirp(t_click, f0=fstart, f1=fend, t1=click_dur, method='log')\n",
    "    print('Selected audio start frequency for chirp is: ' + '{:.2f}'.format(fstart) + ' Hz')\n",
    "    print('Selected audio end frequency for chirp is: ' + '{:.2f}'.format(fend) + ' Hz')\n",
    "    #### THEN LINK audio_fstart TO MAGNITUDE!!!!!!!!!!!!!!\n",
    "    \n",
    "    # with a skew gaussian envelope\n",
    "    t_aux = np.linspace(-1,2,len(t_click))\n",
    "    sigma = 0.65 # spread factor\n",
    "    alpha = 50 # Skewness factor\n",
    "\n",
    "    env = 2*CMMR_class.gaussian(0,sigma,t_aux)*CMMR_class.phi(alpha,t_aux) # skew gaussian\n",
    "    env = env/np.amax(env) # normalize\n",
    "    env_thresh = 0.005; env = env[env>env_thresh] # remove values too close to 0\n",
    "    env = np.interp(t_aux, np.linspace(t_aux[0],t_aux[-1],len(env)), env) # resize array\n",
    "    env = CMMR_class.fade(env, len(env)/100, len(click)/80, 'lin', np.float32) # fade it in & out\n",
    "    \n",
    "    # multiply envelope and click\n",
    "    click = click*env\n",
    "    # normalize\n",
    "    click = .99*click/np.amax(click)\n",
    "    \n",
    "    if CHECKSOUND:\n",
    "        plt.figure(figsize=(16,6))\n",
    "        plt.plot(t_click,click),plt.plot(t_click,env,'r')\n",
    "        plt.xlabel('audio time (s)'), plt.xlim([t_click[0],t_click[-1]])\n",
    "    \n",
    "        librosa.output.write_wav(clicks_dir + 'click_1.wav', resampy.resample(click,audio_working_sr,audio_final_sr), audio_final_sr, norm=False)\n",
    "        \n",
    "elif CLICK == 2.5:\n",
    "    click_suffix = '_ClickDescChirpMag_'\n",
    "\n",
    "elif CLICK == 3:\n",
    "    click_suffix = '_ClickBen_'\n",
    "    click, sr_click = librosa.load(clicks_dir+'click_0.wav')\n",
    "    click = resampy.resample(click,sr_click,audio_working_sr)\n",
    "    click_dur = len(click)/audio_working_sr\n",
    "    t_click = np.arange(0,click_dur,1/audio_working_sr)\n",
    "    click = .99*click/np.amax(click)\n",
    "    \n",
    "    if CHECKSOUND:\n",
    "        plt.figure(figsize=(16,6))\n",
    "        plt.plot(t_click,click)\n",
    "        plt.xlabel('audio time (s)'), plt.xlim([t_click[0],t_click[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lower click amplitude because they'll add up to much more than 1 in amplitude, leading to unbearable distortion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# THIS IS A TENTATIVE NORMALIZATION THING\n",
    "# I TRY TO CALCULATE HOW MANY \"CLICKS\" CAN BE HEARD AT THE SAME TIME IN THE FINAL SOUNDTRACK, AND NORMALIZE ACCORDING TO # IT...\n",
    "data_click_dur = click_dur*speedfactor # (virtual) duration of a click, in the data time domain\n",
    "data_click_len = int(data_click_dur*data_sr) # converted into indices (data domain, still)\n",
    "#data_click_dur = 0.002\n",
    "\n",
    "max_evs_in_win = 0\n",
    "\n",
    "# define search window, that is the duration of a click, in the data time domain\n",
    "indL = 0 # left\n",
    "indR = indL + data_click_len # right\n",
    "\n",
    "DEBUG = 0\n",
    "if DEBUG:\n",
    "    i=0\n",
    "    \n",
    "while indR < len(data_track):\n",
    "    \n",
    "    if DEBUG:\n",
    "        #print(indL)\n",
    "        #print(indR)\n",
    "        pass\n",
    "\n",
    "    n_current_events_in_win = sum(data_track[indL:indR])\n",
    "    #print(n_current_events_in_win)\n",
    "\n",
    "    if DEBUG:\n",
    "        fig = plt.figure()\n",
    "        plt.text(data_t[0],0.7,str(n_current_events_in_win))\n",
    "        plt.plot(data_t,data_track,'k')\n",
    "        plt.plot(data_t[indL:indR],data_track[indL:indR],'r')\n",
    "        fig.savefig('fig' + str(i) + '.png')\n",
    "        i+=1\n",
    "        plt.close(fig)\n",
    "    \n",
    "    max_evs_in_win = max(n_current_events_in_win,max_evs_in_win)\n",
    "\n",
    "    if n_current_events_in_win == 0:\n",
    "        # if no events in current window, directly jump by the length of a click\n",
    "        indL += data_click_len\n",
    "        indR += data_click_len\n",
    "    else:\n",
    "        # else, directly jump to the first event\n",
    "        ind_next_ev = np.where(data_track[indL+1:]==1)[0][0] + indL+1\n",
    "        indL = ind_next_ev\n",
    "        indR = indL + data_click_len\n",
    "    \n",
    "print('With the chosen click sound and the chosen speed factor, they will be at most ' + str(int(max_evs_in_win)) + ' click(s) audible at the same time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click = click/max_evs_in_win\n",
    "if CLICK == 1:\n",
    "    pass\n",
    "elif CLICK == 2:\n",
    "    click = click/22\n",
    "elif CLICK == 3:\n",
    "    click = click/22\n",
    "elif CLICK == 2.5:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize tracks for audio rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tracks (as many as speakers)\n",
    "for k in range(n_speakers):\n",
    "    exec('track_' + str(k) + ' = np.zeros(len(data_t))')\n",
    "    # Audio tracks for now have the same length as the data track\n",
    "    \n",
    "audio_t = np.arange(0,audio_duration,1/audio_working_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill in the audio tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbefore = datetime.datetime.now()\n",
    "\n",
    "for i in range(n_events):\n",
    "#for i in range(1000):\n",
    "    \n",
    "    #print('>> event #' + str(i))\n",
    "    distances = np.zeros(n_speakers) # for current event: distance from each speaker\n",
    "    sum__ = 0 # store the sum of inverse square distances\n",
    "\n",
    "    # 1- distance from current speaker\n",
    "    for k in range(n_speakers):\n",
    "        if LATLON:\n",
    "            distances[k] = CMMR_class.EuclDistance([ev_long[i],ev_lat[i]],[x_speakers[k],y_speakers[k]])\n",
    "        else:\n",
    "            distances[k] = CMMR_class.EuclDistance([ev_X[i],ev_Y[i]],[x_speakers[k],y_speakers[k]])\n",
    "        sum__ = sum__+1/(distances[k]**2)\n",
    "\n",
    "    C = np.sqrt(1/sum__) # normalization constant\n",
    "    #print('C = ' + str(C))\n",
    "    #print('distances = ' + str(distances))\n",
    "\n",
    "    # 2- fill the track with a click at the time of event, amplitude corresponding to ev/speaker distance\n",
    "    for k in range(n_speakers):\n",
    "        if CLICK == 1:\n",
    "            index = int(ev_inds[i])\n",
    "            exec('track_' + str(k) + '[index]' + ' = click*C/distances[k]')\n",
    "        elif CLICK == 2:\n",
    "            start_index = int(ev_inds[i])\n",
    "            end_index = start_index + len(click)\n",
    "            if end_index < len(data_t):\n",
    "                exec('track_' + str(k) + '[start_index:end_index]' + ' = track_' + str(k) + '[start_index:end_index] + click*C/distances[k]')\n",
    "                #plt.plot(click*C/distances[k]+k)\n",
    "                #eval('plt.plot(track_' + str(k) + ')')\n",
    "        elif CLICK == 3:\n",
    "            start_index = int(ev_inds[i])\n",
    "            end_index = start_index + len(click)\n",
    "            if end_index < len(data_t):\n",
    "                exec('track_' + str(k) + '[start_index:end_index]' + ' = track_' + str(k) + '[start_index:end_index] + click*C/distances[k]')\n",
    "        elif CLICK == 2.5:\n",
    "            fmin = 120\n",
    "            fmax = 600\n",
    "\n",
    "            click_dur = 0.2 # in seconds \n",
    "            t_click = np.arange(0,click_dur,1/audio_working_sr)\n",
    "\n",
    "            fstart = CMMR_class.linmap(ev_mag[i],minmag,maxmag,fmax,fmin)\n",
    "            fend = .5*fstart\n",
    "            click = spsig.chirp(t_click, f0=fstart, f1=fend, t1=click_dur, method='log')\n",
    "\n",
    "            t_aux = np.linspace(-1,2,len(t_click))\n",
    "            sigma = 0.65 # spread factor\n",
    "            alpha = 50 # Skewness factor\n",
    "\n",
    "            env = 2*CMMR_class.gaussian(0,sigma,t_aux)*CMMR_class.phi(alpha,t_aux) # skew gaussian\n",
    "            env = env/np.amax(env) # normalize\n",
    "            env_thresh = 0.005; env = env[env>env_thresh] # remove values too close to 0\n",
    "            env = np.interp(t_aux, np.linspace(t_aux[0],t_aux[-1],len(env)), env) # resize array\n",
    "            env = CMMR_class.fade(env, len(env)/100, len(click)/80, 'lin', np.float32) # fade it in & out\n",
    "\n",
    "            # multiply envelope and click\n",
    "            click = click*env\n",
    "            # normalize\n",
    "            click = .99*click/np.amax(click)\n",
    "            click = click/40\n",
    "        \n",
    "            start_index = int(ev_inds[i])\n",
    "            end_index = start_index + len(click)\n",
    "            if end_index < len(data_t):\n",
    "                exec('track_' + str(k) + '[start_index:end_index]' + ' = track_' + str(k) + '[start_index:end_index] + click*C/distances[k]')\n",
    "                \n",
    "tafter = datetime.datetime.now()\n",
    "\n",
    "print('Elapsed time: ' + str(tafter - tbefore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the track(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKTRACK = 0\n",
    "n_track_to_check = 0\n",
    "exec('track_to_check = track_' + str(n_track_to_check))\n",
    "\n",
    "if CHECKTRACK:\n",
    "    fig, ax = plt.subplots(2,figsize=(14,7))\n",
    "    ax[0].plot(data_t,data_track,'k')\n",
    "    #plt.xlim([data_t[0],data_t[-1]])\n",
    "    ax[1].plot(audio_t,track_to_check,'k')\n",
    "    plt.xlim([audio_t[0],audio_t[-1]])\n",
    "    #plt.xlim([ev_times[0],ev_times[3020]])\n",
    "    #plt.xlim([ev_times[0],ev_times[10]])\n",
    "    plt.grid()\n",
    "    ax[1].ticklabel_format(useOffset=False, style='plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audify!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to audio rate & fade in/out\n",
    "for k in range(n_speakers):\n",
    "    exec('track_' + str(k) + ' = resampy.resample(track_' + str(k) + ', audio_working_sr, audio_final_sr)') # resample\n",
    "    exec('track_' + str(k) + ' = CMMR_class.fade(track_' + str(k) + ', 0.3*audio_final_sr,0.3*audio_final_sr,\\'lin\\',np.float32)') # fade in & out\n",
    "    \n",
    "audio_final_t = resampy.resample(audio_t,audio_working_sr,audio_final_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize --> BAD IDEA FOR NOW, constant C already prevents amplitudes from exceeding 1\n",
    "#for k in range(n_speakers):\n",
    "#    exec('track_' + str(k) + '=.9*track_' + str(k) + '/np.amax(np.abs(track_' + str(k) + '))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot if you dare\n",
    "PLOT = 1\n",
    "\n",
    "if PLOT: \n",
    "    plt.figure(figsize=(10,20))\n",
    "    for k in range(n_speakers):\n",
    "        plt.subplot(n_speakers,1,k+1)\n",
    "        eval('plt.plot(audio_final_t,track_' + str(k) + ',\\'k\\')')\n",
    "        #eval('plt.plot(track_' + str(k) + ',\\'k\\')')\n",
    "        plt.grid()\n",
    "        plt.title('Speaker number ' + str(k+1))\n",
    "        plt.ylim([-1,1])\n",
    "        plt.xlim([audio_final_t[0],audio_final_t[-1]])\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('audio time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'Kilauea_K1c_' + str(n_speakers) + 'spks' + click_suffix\n",
    "for k in range(n_speakers):\n",
    "    eval('librosa.output.write_wav(sound_output_dir + base_name + ' + '\\'chan' + str(k+1) + '.wav\\'' + ', track_' + str(k) + ', ' + 'audio_final_sr' + ', norm=False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
